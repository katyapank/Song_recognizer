{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hRVvm2msElDP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pylab, mlab, pyplot\n",
        "import shutil\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Activation, Reshape, Permute\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
        "from tensorflow.keras.layers import GRU, LSTM\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import pathlib\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRWZDE9iGAYR"
      },
      "source": [
        "Извлечение признаков (мел-спектр)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FigwcgE0-KXL",
        "outputId": "28c1b1ab-a4d6-45e6-bf27-7a9d6b9f35a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Содержимое папки 'spectrograms' успешно удалено.\n",
            "Обработаны следующие файлы:\n",
            "7 Seconds -- Youssou N'Dour, Neneh Cherry.mp3\n",
            "a-ha -- Take On Me.mp3\n",
            "ABBA -- Money, Money, Money.mp3\n",
            "ABBA -- The Winner Takes It All.mp3\n",
            "All The Things She Said.mp3\n",
            "Animals — Martin Garrix.mp3\n",
            "Another One Bites The Dust — Queen.mp3\n",
            "Apologize.mp3\n",
            "Appletree.mp3\n",
            "B.o.B, Hayley Williams of Paramore -- Airplanes (feat. Hayley Williams of Paramore).mp3\n",
            "B.o.B, Jessie J -- Price Tag.mp3\n",
            "Bad Bad Boys.mp3\n",
            "Bad Romance.mp3\n",
            "Bag Raiders -- Shooting stars.mp3\n",
            "Данные сохранены в png файлах.\n"
          ]
        }
      ],
      "source": [
        "# предобработка данных\n",
        "def data_preprocessing(audio_file, sr):\n",
        "    audio, sr = librosa.load(audio_file, sr=sr, mono=True)\n",
        "    if audio.ndim == 2:\n",
        "        audio = audio.T\n",
        "    audio = audio.astype(np.float32)\n",
        "    max_val = np.max(np.abs(audio))\n",
        "    if max_val > 0:\n",
        "        audio /= max_val\n",
        "    return audio\n",
        "\n",
        "\n",
        "# создание транспонированной матрицы мел-спектра\n",
        "def create_mel_spectrogram(audio_file, sr, hop_length, fragment_size_ms, layering):\n",
        "    data = data_preprocessing(audio_file, sr)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_fft=512, hop_length=hop_length)\n",
        "    t_spectrogram = np.transpose(librosa.power_to_db(mel_spectrogram, ref=np.max))\n",
        "    return t_spectrogram\n",
        "\n",
        "\n",
        "def save_spectrogram(data, filename):\n",
        "    pylab.figure(figsize=(data.shape[1], data.shape[0]), dpi=1)\n",
        "    pylab.axis('off') # no axis\n",
        "    axes = pylab.gca()\n",
        "    axes.set_ylim([-80.0,0])\n",
        "    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
        "    librosa.display.specshow(data)\n",
        "    pylab.savefig(filename, bbox_inches=None, pad_inches=0, format=\"png\")\n",
        "    pylab.clf()\n",
        "    pylab.close()\n",
        "\n",
        "\n",
        "# использование всех аудиофайлов в директории\n",
        "filepath = '../songs'\n",
        "audio_files = glob.glob(os.path.join(filepath, '*.mp3')) + glob.glob(os.path.join(filepath, '*.wav'))\n",
        "\n",
        "output_folder = 'spectrograms'\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "else:\n",
        "    shutil.rmtree(output_folder)\n",
        "    print(\"Содержимое папки 'spectrograms' успешно удалено.\")\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "print('Обработаны следующие файлы:')\n",
        "for file in audio_files:\n",
        "    print(os.path.basename(file))\n",
        "    data = create_mel_spectrogram(file, 16000, 512, 1000, 1)\n",
        "\n",
        "    filename = os.path.splitext(os.path.basename(file))[0]\n",
        "    output_file = os.path.join(output_folder, f\"{filename}.png\")\n",
        "    save_spectrogram(data, output_file)\n",
        "\n",
        "print('Данные сохранены в png файлах.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQdn7jZtDRF7"
      },
      "source": [
        "Используемая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "2GeAb4Vs7AGY"
      },
      "outputs": [],
      "source": [
        "def CRNN2D(IMG_SHAPE, nb_classes):\n",
        "    '''\n",
        "    Model used for evaluation in paper. Inspired by K. Choi model in:\n",
        "    https://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py\n",
        "    '''\n",
        "\n",
        "    nb_layers = 4  # number of convolutional layers\n",
        "    nb_filters = [64, 128, 128, 128]  # filter sizes\n",
        "    kernel_size = (3, 3)  # convolution kernel size\n",
        "    activation = 'elu'  # activation function to use after each layer\n",
        "    #pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n",
        "    #             (4, 2)]  # size of pooling area\n",
        "\n",
        "    pool_size = [(2, 2), (2, 4), (2, 4), (2, 4),\n",
        "                 (2, 4)]  # size of pooling area\n",
        "    # shape of input data (frequency, time, channels)\n",
        "    input_shape = (IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2])\n",
        "    frequency_axis = 2\n",
        "    time_axis      = 1\n",
        "    channel_axis   = 3\n",
        "\n",
        "    #print(input_shape)\n",
        "\n",
        "    # Create sequential model and normalize along frequency axis\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n",
        "\n",
        "    # First convolution layer specifies shape\n",
        "    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n",
        "                     data_format=\"channels_last\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Activation(activation))\n",
        "    model.add(BatchNormalization(axis=channel_axis))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Add more convolutional layers\n",
        "    for layer in range(nb_layers - 1):\n",
        "        # Convolutional layer\n",
        "        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size,\n",
        "                         padding='same'))\n",
        "        model.add(Activation(activation))\n",
        "        model.add(BatchNormalization(\n",
        "            axis=channel_axis))  # Improves overfitting/underfitting\n",
        "        model.add(MaxPooling2D(pool_size=pool_size[layer + 1],\n",
        "                               strides=pool_size[layer + 1]))  # Max pooling\n",
        "        model.add(Dropout(0.1))\n",
        "\n",
        "        # Reshaping input for recurrent layer\n",
        "    # (frequency, time, channels) --> (time, frequency, channel)\n",
        "    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n",
        "    resize_shape = model.output_shape[2] * model.output_shape[3]\n",
        "    model.add(Reshape((model.output_shape[1], resize_shape)))\n",
        "\n",
        "    # recurrent layer\n",
        "    model.add(GRU(32, return_sequences=True))\n",
        "    model.add(GRU(32, return_sequences=False))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    #print(model.output_shape)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXIwKLpMEcWq"
      },
      "source": [
        "Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество композиций: 14\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
            "Размер изображений:\n",
            "(6339, 128)\n",
            "train_size:  9\n",
            "val_size:  2\n",
            "test_size:  2\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "r_seed = 777\n",
        "\n",
        "# получение спика изображений\n",
        "filepath = 'spectrograms'\n",
        "spectrogram_files = glob.glob(os.path.join(filepath, '*.png'))\n",
        "\n",
        "# перемешивание списка изображений\n",
        "random.Random(r_seed).shuffle(spectrogram_files)\n",
        "\n",
        "# получение количества изображений (классов)\n",
        "classes_number = len(audio_files)\n",
        "print('Количество композиций:', classes_number)\n",
        "\n",
        "# нумерация классов\n",
        "label_to_index = dict((name, index) for index, name in enumerate(spectrogram_files))\n",
        "# all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
        "#                             for path in spectrogram_files]\n",
        "all_image_labels = list(label_to_index.values())\n",
        "print(all_image_labels)\n",
        "\n",
        "# one-hot-encoding\n",
        "def to_uni(pos, lenof):\n",
        "    res = [0. for i in range(lenof)]\n",
        "    res[pos] = 1.\n",
        "    return res\n",
        "\n",
        "all_image_labels = [to_uni(i, classes_number) for i in all_image_labels]\n",
        "print(all_image_labels)\n",
        "\n",
        "# получение формы входного изображения.\n",
        "print('Размер изображений:')\n",
        "first_shape = tf.image.decode_png(tf.io.read_file(spectrogram_files[0])).shape\n",
        "pixels = (first_shape[0], first_shape[1])\n",
        "print(pixels)\n",
        "\n",
        "# создание датасета\n",
        "ds = tf.data.Dataset.from_tensor_slices((spectrogram_files,\n",
        "                                                 all_image_labels))\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, pixels)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image)\n",
        "\n",
        "\n",
        "def load_and_preprocess_from_path_label(path, label):\n",
        "    return load_and_preprocess_image(path), label\n",
        "\n",
        "\n",
        "image_label_ds = ds.map(load_and_preprocess_from_path_label)\n",
        "inp_shape = image_label_ds.element_spec[0].shape\n",
        "image_label_ds = image_label_ds.batch(batch_size)\n",
        "\n",
        "train_size = int(0.70 * classes_number / np.float64(batch_size))\n",
        "val_size   = int(0.15 * classes_number / np.float64(batch_size))\n",
        "test_size  = int(0.15 * classes_number / np.float64(batch_size))\n",
        "\n",
        "print(\"train_size: \", train_size, flush=True)\n",
        "print(\"val_size: \", val_size, flush=True)\n",
        "print(\"test_size: \",test_size, flush=True)\n",
        "\n",
        "train_dataset = image_label_ds.take(train_size)\n",
        "test_dataset = image_label_ds.skip(train_size)\n",
        "val_dataset = test_dataset.take(val_size)\n",
        "test_dataset  = test_dataset.skip(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_folder = 'weights'\n",
        "if not os.path.exists(weights_folder):\n",
        "    os.makedirs(weights_folder)\n",
        "else:\n",
        "    shutil.rmtree(weights_folder)\n",
        "    print(\"Содержимое папки 'weights' успешно удалено.\")\n",
        "    os.makedirs(weights_folder)\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "\n",
        "model = CRNN2D(inp_shape, nb_classes = classes_number)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate=learning_rate), metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=weights_folder + \"/model.keras\", verbose=1, save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='loss', min_delta=1, patience=10, verbose=0, mode='auto')\n",
        "callbacks = [checkpointer]\n",
        "# if (is_early_stop):\n",
        "callbacks.append(earlystopper)\n",
        "\n",
        "\n",
        "history = model.fit(train_dataset, validation_data = val_dataset, epochs = epochs, batch_size = batch_size, callbacks = callbacks)\n",
        "model.save(\"weights/model.keras\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
